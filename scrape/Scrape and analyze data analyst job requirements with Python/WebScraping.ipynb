{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from typing import Dict, Optional\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generating a URL with a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated URL: https://www.linkedin.com/jobs/search?keywords=Data%20Analyst&location=New%20York\n"
     ]
    }
   ],
   "source": [
    "def generate_linkedin_url(position: str, location: str) -> str:\n",
    "    \"\"\"Generate LinkedIn URL for job search\"\"\"\n",
    "    position = position.replace(' ', '%20')\n",
    "    location = location.replace(' ', '%20')\n",
    "    return f\"https://www.linkedin.com/jobs/search?keywords={position}&location={location}\"\n",
    "\n",
    "# Test the function\n",
    "test_url = generate_linkedin_url(\"Data Analyst\", \"New York\")\n",
    "print(f\"Generated URL: {test_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract the Job Data from a single job posting card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_linkedin_job_data(job_card: BeautifulSoup, headers: dict) -> Optional[Dict]:\n",
    "    \"\"\"Extract job data from a LinkedIn job card\"\"\"\n",
    "    try:\n",
    "        # Extract basic job information\n",
    "        title = job_card.find('h3', class_='base-search-card__title').text.strip()\n",
    "        company = job_card.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "        location = job_card.find('span', class_='job-search-card__location').text.strip()\n",
    "        \n",
    "        # Extract job link\n",
    "        job_link = job_card.find('a', class_='base-card__full-link')['href']\n",
    "        \n",
    "        # Extract posting date\n",
    "        date_posted = job_card.find('time')['datetime'] if job_card.find('time') else None\n",
    "        \n",
    "        # Get job description from the job detail page\n",
    "        description = \"\"\n",
    "        try:\n",
    "            # Add delay before requesting job details\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            # Make request to job detail page\n",
    "            job_response = requests.get(job_link, headers=headers)\n",
    "            \n",
    "            if job_response.status_code == 200:\n",
    "                job_soup = BeautifulSoup(job_response.text, 'html.parser')\n",
    "                \n",
    "                # Find the job description\n",
    "                description_div = job_soup.find('div', class_='show-more-less-html__markup')\n",
    "                if description_div:\n",
    "                    description = description_div.get_text(strip=True)\n",
    "                else:\n",
    "                    # Try alternative class names\n",
    "                    description_div = job_soup.find('div', class_='description__text')\n",
    "                    if description_div:\n",
    "                        description = description_div.get_text(strip=True)\n",
    "            else:\n",
    "                logger.warning(f\"Could not fetch job description. Status code: {job_response.status_code}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error fetching job description: {e}\")\n",
    "        \n",
    "        # Create job data dictionary\n",
    "        job_data = {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'location': location,\n",
    "            'job_link': job_link,\n",
    "            'date_posted': date_posted,\n",
    "            'description': description\n",
    "        }\n",
    "        \n",
    "        return job_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting job data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_linkedin_jobs(position: str, location: str) -> list:\n",
    "    \"\"\"Main function to scrape LinkedIn jobs\"\"\"\n",
    "    \n",
    "    # Initialize variables\n",
    "    jobs_list = []\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Generate URL\n",
    "        url = generate_linkedin_url(position, location)\n",
    "        \n",
    "        # Add delay to prevent rate limiting\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        \n",
    "        # Make request to LinkedIn\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Parse the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all job cards\n",
    "            job_cards = soup.find_all('div', class_='base-card')\n",
    "            \n",
    "            # Extract data from each job card\n",
    "            for card in job_cards:\n",
    "                job_data = extract_linkedin_job_data(card, headers)\n",
    "                if job_data:\n",
    "                    jobs_list.append(job_data)\n",
    "                    logger.info(f\"Successfully scraped job: {job_data['title']} at {job_data['company']}\")\n",
    "            \n",
    "            logger.info(f\"Successfully scraped {len(jobs_list)} valid jobs from LinkedIn\")\n",
    "        else:\n",
    "            logger.error(f\"LinkedIn returned status code: {response.status_code}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while scraping: {e}\")\n",
    "    \n",
    "    return jobs_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Describe Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 00:14:11,054 - ERROR - LinkedIn returned status code: 429\n"
     ]
    }
   ],
   "source": [
    "# Test the scraping\n",
    "position = \"Data Analyst\"\n",
    "location = \"New York\"\n",
    "jobs = scrape_linkedin_jobs(position, location)\n",
    "\n",
    "# Save results\n",
    "if jobs:\n",
    "    filename = f\"job_listings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    save_to_csv(jobs, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Notes\n",
    "\n",
    "1. **Functionality**:\n",
    "   - The scraper successfully extracts key job information including:\n",
    "     - Job title\n",
    "     - Company name\n",
    "     - Location\n",
    "     - Job posting URL\n",
    "     - Date posted\n",
    "\n",
    "2. **Limitations**:\n",
    "   - LinkedIn has rate limiting (429 errors)\n",
    "   - Only scrapes the first page of results\n",
    "   - Requires careful handling of request delays\n",
    "\n",
    "3. **Best Practices Implemented**:\n",
    "   - Error handling at each step\n",
    "   - Logging for debugging\n",
    "   - Random delays between requests\n",
    "   - User-Agent headers to mimic browser\n",
    "   - Type hints for better code documentation\n",
    "\n",
    "4. **Potential Improvements**:\n",
    "   - Add pagination to get more results\n",
    "   - Implement proxy rotation\n",
    "   - Add more job details from individual job pages\n",
    "   - Add data cleaning and analysis\n",
    "   - Implement retry mechanism for failed requests\n",
    "\n",
    "5. **Usage Notes**:\n",
    "   - Wait between scraping sessions to avoid rate limiting\n",
    "   - Consider using LinkedIn's official API for production use\n",
    "   - Respect robots.txt and website terms of service"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
